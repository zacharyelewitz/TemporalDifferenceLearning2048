# Reinforcement Learning Agent Playing 2048

I love the sliding block puzzle game [2048](https://2048game.com/). It's fun to see what the largest tile you can get is (my personal best is 16,384), which strategies are most resilient to mistakes, and sometimes it's entertaining to purposefully swipe in the wrong direction to see if you're capable enough to get the board back into good shape.

After a few months of playing I became curious as to whether I could train a computer to play the game. At first I built an expectimax agent to play the game but this wasn't any fun because the expectimax algorithm effectively just looks many steps ahead to see what performs best over all possible moves and that felt like cheating. I then considered utilizing Q-tables to train a reinforcement learning agent to play the game but the number of board states was too high to be a reasonable solution. I knew that I needed an approach that considered collections of contiguous segments of the board at once. 

This lead me to temporal difference learning with collections of n-tuple networks. Full details and background regarding this approach may be found [here](http://storage.kghost.de/cig_proc/full/paper_88.pdf). The main idea is instead of having one large Q-table, there are look up tables, LUTs, (similar to small subsets of Q-tables) that associate certain tiles of the board with rewards by swiping in a particular direction. The direction to swipe next is determined by the overall reward over all of the LUTs. These LUTs are updated by looking a step ahead at the possible point gains by moving in each direction. By only considering subsets of the board the number of states in the LUTs is manageable but they still capture the key relationships between the tiles in order to determine which move is best. Updating the LUTs over playing hundreds of thousands of games teaches the game to consider long term rewards instead of focusing on quick point gains. I specifically used the TD-Afterstate algorithm noted in the paper.

I originally did this project in python but found it to be quite slow. I took this as an opportunity to learn Go and training the agent became significantly faster. It was also interesting to recreate the project without the help of the python/numpy functionality for working with matrices.
